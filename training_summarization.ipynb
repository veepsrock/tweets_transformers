{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73491f9d67324c87a1b06030a4f76b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7ae7de16c84ba1b610ad6e803aa935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327f5a2dde8d427ab03b45a98bc5e016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samsum downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5032578d6c5040d49e9a022a2b4245cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him üôÇ\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5349f4dd2efc48a0ad458a9418c28346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0c3e54702647448c55e5ba8d5e52d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae44ad40cd4015a1ea62fe0196cb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c71f023f1c4e5ca680670b539c0079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ec3285fd944acb9065d553058c6e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"google/pegasus-cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df56a8a965c4bb3b2e623bcb8ff62a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñç        | 15/103 [23:24<2:19:51, 95.35s/it]"
     ]
    }
   ],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                                   tokenizer, column_text=\"dialogue\",\n",
    "                                   column_summary=\"summary\", batch_size=8)\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-5rcjawmr because the default path (/home/ubuntu/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fd468e031b4a0aa55cc02b1e959d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d388b39c62e943d48fe35b4180ac7cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d632ae4194b7a9ca9ad95dbcd8a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\n",
    "                                truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n",
    "                                     truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
    "                                       batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to huggin face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Login successful\n",
      "Your token has been saved to /home/ubuntu/.huggingface/token\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m             lfs_version = run_subprocess(\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0;34m\"git-lfs --version\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             ).stdout.strip()\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/huggingface_hub/utils/_subprocess.py\u001b[0m in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs': 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8488/769651858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                   \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq2seq_data_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_samsum_pt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                   eval_dataset=dataset_samsum_pt[\"validation\"])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   2945\u001b[0m                 \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2947\u001b[0;31m                 \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_private_repo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2948\u001b[0m             )\n\u001b[1;32m   2949\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_git_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bertopic_gpu/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             raise EnvironmentError(\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0;34m\"Looks like you do not have git-lfs installed, please install.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;34m\" You can install from https://git-lfs.github.com/.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;34m\" Then run `git lfs install` (you only have to do this once).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic_gpu",
   "language": "python",
   "name": "bertopic_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
